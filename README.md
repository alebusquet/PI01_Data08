
------------

# HENRY LABS

------------

# PROYECTO INDIVIDUAL 01
## DATA ENGINEERING & MACHINE LEARNING
### Alejandro Busquet
------------

------------



# PARTE 1: DATA ENGINEERING

## Tem√°tica:

Empec√© a trabajar como Data Scientist en una start-up que provee servicios de agregaci√≥n de plataformas de streaming, para lo cual deber√© crear mi primer modelo de ML, que solucione un problema de negocio: un sistema de recomendaci√≥n que a√∫n no ha sido puesto en marcha!

Voy a los datos y me doy cuenta que la madurez de los mismos es poca. Datos sin transformar, no hay procesos automatizados para la actualizaci√≥n de nuevas pel√≠culas o series, entre otras cosas...

Debo comenzar desde 0, haciendo un trabajo r√°pido de Data Engineer y tener un MVP (Minimum Viable Product) para la pr√≥xima semana! Mi cabeza va a explotar ü§Ø, pero al menos s√© cual es, conceptualmente, el camino que debo de seguir. As√≠ que trato de espantar los miedos y me pongo manos a la obra üí™

## Herramientas:

Para trabajar cuento con 4 datasets correspondiente a 4 plataformas de streaming: Amazon Prime, Disney Plus, Hulu y Netflix, con variada informacion sobre las pel√≠culas o temporadas.

Por otro lado cuento con 8 datasets adicionales que contienen informaci√≥n referente a los usuarios y sus preferencias.

## Propuesta de trabajo:

- Generar un campo id: Cada id se compondr√° de la primera letra del nombre de la plataforma, seguido del show_id ya presente en los datasets (ejemplo para t√≠tulos de Amazon = **a**s123)

- Reemplazo de nulos: Los valores nulos de la columna "rating" deber√°n reemplazarse por el string ‚ÄúG‚Äù (que corresponde al maturity rating: ‚Äúgeneral for all audiences‚Äù)

- Fechas: De haber fechas, deber√°n tener el formato AAAA-mm-dd

- Texto: Los campos de texto deber√°n estar todos en min√∫sculas, sin excepciones

- Columna "duration": Este campo contiene un n√∫mero y una unidad de medici√≥n. Deber√° abrirse en dos campos: "duration_int" y "duration_type". El primero ser√° un integer que contendr√° la duraci√≥n, y el segundo un string que indicar√° la unidad de medici√≥n de esa duraci√≥n ("min" = minutos o "season" = temporadas)

## Objetivos:

1. En la funci√≥n de Data Engineer, el Tech Lead solicita realizar un proceso de ETL (extracci√≥n, transformaci√≥n y carga) sobre los cuatro datasets proporcionados. Esto con el objetivo de contar con una base lo mas "limpia" posible, que luego permita extraer informaci√≥n valiosa referente tanto a las plataformas como a los cat√°logos de pel√≠culas y series que en ellas se ofrece.

2. Se me encarga elaborar una API, a efectos de disponibilizar todos los datos de manera online. Esta API permitir√° que la informaci√≥n de la base de datos pueda ser accedida mediante, en este caso, cuatro consultas predefinidas.

3. Como objetivo final para este m√≥dulo, se me encarg√≥ documentar todo este proceso, subir los archivos a github y confeccionar un video donde se explicar√≠an los pasos realizados y se mostrar√≠a el funcionamiento de los distintos procesos.

## Funcionalidades de la API:

Con el prop√≥sito de disponibilizar los datos de la empresa, usando un framework como FastAPI, las consultas que deber√° responder son las siguientes:

- Pel√≠cula con mayor duraci√≥n, con filtros opcionales de A√ëO, PLATAFORMA Y TIPO DE DURACI√ìN (la funci√≥n debe llamarse "get_max_duration(year, platform, duration_type)")

- Cantidad de pel√≠culas por plataforma, con un puntaje mayor a XX en determinado a√±o (la funci√≥n debe llamarse "get_score_count(platform, scored, year")

- Cantidad de pel√≠culas por plataforma, con filtro de PLATAFORMA. (la funci√≥n debe llamarse "get_count_platform(platform)")

- Actor que m√°s se repite, seg√∫n plataforma y a√±o (la funci√≥n debe llamarse "get_actor(platform, year)")

**Hasta ac√° trat√© lo referente a las tareas, las propuestas y los objetivos del trabajo, paso a definir las herramientas utilizadas y los pasos efectuados.
**

------------


## Herramientas utilizadas:

- Python: es el lenguaje de programaci√≥n utilizado, ya que brinda acceso a las librer√≠as apropiadas para realizar la tarea encomendada, de una manera clara, eficaz y eficiente.

- Pandas: framework que representa el complemento perfecto para Python, ya que permite acceder a los archivos csv provistos, su conversi√≥n a dataframes, el proceso. de transformaci√≥n de los datos y la posterior exportaci√≥n de los mismos a un √∫nico archivo, el cual luego ser√° utilizado por la API para disponibilizar los datos.

- Pandasql: librer√≠a utilizada en el back, que nos permiti√≥ efectuar consultas con lenguaje SQL, sobre los Dataframes de Pandas.

- FastApi: framework para la construcci√≥n de la API basada en Python.

- Uvicorn: permite efectuar tests para controlar el funciomaniento de la API de manera local, previo a su despliegue en Render. Maneja tambi√©n la actualizaci√≥n autom√°tica de los archivos de uso, cuando estos son modificados por correcciones, permitiendo una continuidad en dichas pruebas.

- Render: plataforma online y gratuita que permite, mediante una direcci√≥n web, disponibilizar tanto la API como los datos, para que puedan ser accedidos y consultados por el usuario mediante su software. Aqu√≠ el [link](https://render.com/docs/free#free-web-services "link") y el [tutorial](https://github.com/HX-FNegrete/render-fastapi-tutorial "tutorial").

## Pasos efectuados:

Descargue los cuatro datasets originales y realic√© una previsualizaci√≥n de los datos.
Utilizando el Jupyter notebook, import√© las librer√≠as necesarias para desempe√±ar los pasos.

Comenc√© a trabajar en el ETL, enfocado principalmente en la "Propuesta de trabajo" (detallada arriba), pero tambi√©n en otros puntos adicionales como la verificaci√≥n de filas repetidas, eliminaci√≥n de datos nulos, correcci√≥n de datos en campos de texto y dem√°s.

Una vez que tuve disponible una base de datos "limpia", encar√© la tarea de las Funciones para la API, creando el archivo "main.py" y realizando las pruebas de manera local mediante las herramientas FastAPI y Uvicorn.

Una vez realizadas algunas tareas extra, y ya depurados los procesos que permit√≠an realizar las consultas encomendadas, sub√≠ los archivos y las bibliotecas a Github.

Pas√© entonces al proceso del deploy de la API por medio de Render, para lo que tuve que poner a punto algunas cuestiones t√©cnicas, hasta que pude acceder a un dominio y experimentar con diferentes consultas.

## PARTE 2: MACHINE LEARNING

Una vez que los datos son consumibles desde la API y la misma se encuentra lista para ser accedida por los departamentos de Analytics y de Machine Learning, y tenemos un EDA bien realizado, es hora de entrenar nuestro modelo de Machine Learning para que sea capaz de armar un sistema de recomendaci√≥n de pel√≠culas para usuarios, por el cual dado un id de usuario y una pel√≠cula, el modelo nos diga si la recomienda o no para dicho usuario.

De ser posible, este sistema de recomendaci√≥n debe ser deployado para tener una interfaz gr√°fica amigable para ser utilizada, usando para su deployment "Gradio" o alguna soluci√≥n como "Streamlit" u otra similar en local (lograr el deployment del sistema de recomendaci√≥n o una interfaz gr√°fica son un plus al proyecto).

Para esta etapa dispuse del archivo ya depurado en la primera parte, por lo que proced√≠ a encarar el EDA, correlaciones y outliers.

Luego, utilizando la librer√≠a "Surprise", especializada en este arte, pude proporcionarle (mediante el "reader" y "data") los datos que extraje de los 8 datasets, con informaci√≥n valiosa de los usuarios, para que mediante un modelo SVD (Singular Value Decomposition) me devuelva una estimaci√≥n sobre la recomendaci√≥n o no de determinada pel√≠cula a ese usuario.

Para medir la validez de esa estimaci√≥n, utilic√© las m√©tricas del RMSE (Root Mean Square Error), MAE (Mean Absolute Error) y las de Precisi√≥n, Recall y F1-score.

## Conclusi√≥n:

Por √∫ltimo pas√© a grabar, editar y publicar el video, en el cual se muestra a la API en funcionamiento, efectuando algunas consultas de muestra. Se accede mediante este [link al video](https://youtu.be/f0zM91a6HRM "link al video").

Para terminar, document√© todos los pasos realizados en este Readme.md, para disponibilidad p√∫blica.

Muchas gracias!

##### Autor: Alejandro Busquet
##### Correo: abusquet@hotmail.com / algabu00@gmail.com

